# -*- coding: utf-8 -*-
"""Model_take_3.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1c1M-0a5fXgiYnYjNa3CoOUxUKgBmoXcB
"""

import pandas as pd
from google.colab import drive
drive.mount('/content/drive')

!pip install https://s3-us-west-2.amazonaws.com/ai2-s2-scispacy/releases/v0.5.1/en_core_sci_sm-0.5.1.tar.gz
from tensorflow.keras.models import Sequential
from tensorflow.keras.layers import Embedding, LSTM, Dense, Dropout
import matplotlib.pyplot as plt
from tensorflow.keras.preprocessing.text import Tokenizer
from tensorflow.keras.preprocessing.sequence import pad_sequences
from sklearn.model_selection import train_test_split
from tensorflow.keras.models import Sequential
from tensorflow.keras.layers import Embedding, Flatten, Dense
import spacy
import numpy as np
import pandas as pd

data = pd.read_csv('/content/drive/MyDrive/Mtech /Dissertation/data_output/raw_data_converted_without_fp.csv')
mask = data['status'] == 'completed'
data['label'] = mask.map({True: 1, False: 0})
data= data.fillna("")
nlp = spacy.load("en_core_sci_sm")

data.columns

data['combined_text'] = data['why_stop_updated'] + ' ' + data['diseases_updated'] + ' ' + data['drugs_updated'] + ' ' + data['criteria_updated']

data['label']

tokens = []
for text in data['combined_text']:
    doc = nlp(text)
    token_list = [token.text for token in doc]
    tokens.append(token_list)

from gensim.models import Word2Vec

word2vec_model = Word2Vec(tokens, vector_size=500, window=5, min_count=1, sg=0)

word_embeddings = word2vec_model.wv

# Calculate vocab_size as the number of unique tokens
vocab_size = len(word_embeddings.key_to_index)

# Determine embedding_dim based on the dimensionality of your word embeddings
embedding_dim = word2vec_model.vector_size

# Find max_sequence_length as the maximum length of tokenized sequences
max_sequence_length = max(len(tokens) for tokens in tokens)

# Create an embedding_matrix with word embeddings for your vocabulary
embedding_matrix = word_embeddings.vectors  # Use the vectors attribute to get the matrix

X_train, X_val_test, y_train, y_val_test = train_test_split(tokens, data['label'], test_size=0.3, random_state=42)
X_val, X_test, y_val, y_test = train_test_split(X_val_test, y_val_test, test_size=0.5, random_state=42)

# Create a tokenizer and fit it on your training data
tokenizer = Tokenizer(num_words=vocab_size)
tokenizer.fit_on_texts(X_train)

# Convert text sequences to sequences of word indices
X_train_seq = tokenizer.texts_to_sequences(X_train)
X_val_seq = tokenizer.texts_to_sequences(X_val)
X_test_seq = tokenizer.texts_to_sequences(X_test)

# Pad or truncate sequences to a fixed length (max_sequence_length)
X_train_padded = pad_sequences(X_train_seq, maxlen=max_sequence_length, padding='post', truncating='post')
X_val_padded = pad_sequences(X_val_seq, maxlen=max_sequence_length, padding='post', truncating='post')
X_test_padded = pad_sequences(X_test_seq, maxlen=max_sequence_length, padding='post', truncating='post')



model = Sequential()
model.add(Embedding(input_dim=vocab_size, output_dim=embedding_dim, input_length=max_sequence_length, weights=[embedding_matrix], trainable=False))
model.add(Flatten())
model.add(Dense(128, activation='relu'))
model.add(Dense(1, activation='sigmoid'))

# Compile the model
model.compile(optimizer='adam', loss='binary_crossentropy', metrics=['accuracy'])


history = model.fit(X_train_padded, y_train, epochs=10, validation_data=(X_val_padded, y_val))

loss, accuracy = model.evaluate(X_test_padded, y_test)
print(f"Test Loss: {loss}, Test Accuracy: {accuracy}")

plt.figure(figsize=(12, 4))
plt.subplot(1, 2, 1)
plt.plot(history.history['loss'], label='Training Loss')
plt.plot(history.history['val_loss'], label='Validation Loss')
plt.xlabel('Epochs')
plt.ylabel('Loss')
plt.legend()

plt.subplot(1, 2, 2)
plt.plot(history.history['accuracy'], label='Training Accuracy')
plt.plot(history.history['val_accuracy'], label='Validation Accuracy')
plt.xlabel('Epochs')
plt.ylabel('Accuracy')
plt.legend()

plt.show()

