# -*- coding: utf-8 -*-
"""Model_take1.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1YQWUtCRuT1HZQNd2mjwp-bi-zorqGS5g
"""

import pandas as pd
from google.colab import drive
drive.mount('/content/drive')

import tensorflow as tf
from tensorflow.keras.preprocessing.text import Tokenizer
from tensorflow.keras.preprocessing.sequence import pad_sequences
from tensorflow.keras.models import Sequential
from tensorflow.keras.layers import Embedding, LSTM, Dense, Dropout, Bidirectional
import matplotlib.pyplot as plt

from sklearn.model_selection import train_test_split

data = pd.read_csv('//content/drive/MyDrive/Mtech /Dissertation/data_output/raw_data.csv')

data.columns

mask = data['status'] == 'completed'
data['label'] = mask.map({True: 1, False: 0})

use_data= use_data.fillna("")
use_data['combine_data'] = use_data['why_stop_updated'] + "  " + use_data["diseases_updated"] + "  " + use_data["drugs_updated"] + use_data["criteria_updated"]

use_data

tokenizer = Tokenizer(num_words = 5000)
tokenizer.fit_on_texts(use_data['combine_data'])  # Replace 'diseases' with the appropriate column name
X_text = tokenizer.texts_to_sequences(use_data['combine_data'])  # Convert text to sequences
X_text = pad_sequences(X_text, maxlen=500)  # Pad sequences if needed

X_train, X_test, y_train, y_test = train_test_split(X_text, use_data['label'], test_size=0.4, random_state=42, stratify = use_data['label'])

X_train.shape, X_test.shape, y_train.shape

vocab_size = len(tokenizer.word_index) + 1
embedding_dim = 300
max_sequence_length = X_train.shape[1]

model = Sequential()
model.add(Embedding(input_dim=vocab_size, output_dim=embedding_dim, input_length=max_sequence_length))

model.add(Bidirectional(LSTM(units=64, return_sequences=True)))
model.add(Bidirectional(LSTM(units=64, return_sequences=True)))

model.add(Dense(units=64, activation='relu'))
model.add(Dropout(0.5))
model.add(Dense(units=1, activation='sigmoid'))

model.compile(optimizer='adam', loss='binary_crossentropy', metrics=['accuracy'])

model.summary()

history = model.fit(X_train, y_train, epochs=10, batch_size=64, validation_split=0.2)

# Plot the training and validation metrics
plt.figure(figsize=(12, 4))
plt.subplot(1, 2, 1)
plt.plot(history.history['loss'], label='Training Loss')
plt.plot(history.history['val_loss'], label='Validation Loss')
plt.xlabel('Epoch')
plt.ylabel('Loss')
plt.legend()

# Plot the training and validation accuracy
plt.subplot(1, 2, 2)
plt.plot(history.history['accuracy'], label='Training Accuracy')
plt.plot(history.history['val_accuracy'], label='Validation Accuracy')
plt.xlabel('Epoch')
plt.ylabel('Accuracy')
plt.legend()

plt.tight_layout()
plt.show()

loss, accuracy = model.evaluate(X_test, y_test)
print(f"Test Loss: {loss}, Test Accuracy: {accuracy}")